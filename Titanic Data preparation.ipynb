{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling Helpers\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, ShuffleSplit, StratifiedShuffleSplit\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.preprocessing import Imputer, Normalizer, scale, StandardScaler\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.externals.joblib import parallel_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets: full: (1309, 12) train set: (891, 12)\n"
     ]
    }
   ],
   "source": [
    "# get titanic & test csv files as a DataFrame\n",
    "train = pd.read_csv(\"C:/Users/Eugene/Desktop/Output/Titanic/train.csv\")\n",
    "test  = pd.read_csv(\"C:/Users/Eugene/Desktop/Output/Titanic/test.csv\")\n",
    "ids = test['PassengerId']\n",
    "full = train.append( test , ignore_index = True )\n",
    "print ('Datasets:' , 'full:' , full.shape , 'train set:' , train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name True\n",
      "PassengerId True\n",
      "Survived True\n",
      "Pclass True\n",
      "Sex True\n",
      "Age False\n",
      "SibSp True\n",
      "Parch True\n",
      "Ticket True\n",
      "Fare False\n",
      "Cabin False\n",
      "Embarked False\n"
     ]
    }
   ],
   "source": [
    "# Data  check\n",
    "print('Name', full[pd.isnull(full['Name'])==True].empty)\n",
    "print('PassengerId', full[pd.isnull(full['PassengerId'])==True].empty)\n",
    "print('Survived', train[pd.isnull(train['Survived'])==True].empty)\n",
    "print('Pclass', full[pd.isnull(full['Pclass'])==True].empty)\n",
    "print('Sex', full[pd.isnull(full['Sex'])==True].empty)\n",
    "print('Age', full[pd.isnull(full['Age'])==True].empty)\n",
    "print('SibSp', full[pd.isnull(full['SibSp'])==True].empty)\n",
    "print('Parch', full[pd.isnull(full['Parch'])==True].empty)\n",
    "print('Ticket', full[pd.isnull(full['Ticket'])==True].empty)\n",
    "print('Fare', full[pd.isnull(full['Fare'])==True].empty)\n",
    "print('Cabin', full[pd.isnull(full['Cabin'])==True].empty)\n",
    "print('Embarked', full[pd.isnull(full['Embarked'])==True].empty)\n",
    "# This allows to identify that Age,Cabin,Embark fields require some additional improvement (unless dropped)\n",
    "# But we won't drop them because there are correlations between survival and age and cabin (deck) so we will\n",
    "# try to do our best to fill them in\n",
    "#It appeares that some of these tickets are group tickets, meaning that Fare per passenger should be ajusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeating_tickets = pd.Series.value_counts(full['Ticket'])[pd.Series.value_counts(full['Ticket'])>1]\n",
    "ticket_tags=repeating_tickets.index.tolist()\n",
    "repeating_surnames=pd.Series.value_counts(train.Name.str.extract('([A-Z][a-z]*)\\,', expand=False)).index.tolist()\n",
    "for ticket_tag in ticket_tags:\n",
    "    full.loc[full['Ticket'] == ticket_tag, ['Fare']] = full[full['Ticket'] == ticket_tag]['Fare'].mul(1/repeating_tickets[ticket_tag])\n",
    "# category Names contains too much detail, lets take out the essential: Family surname (to buld family variable)\n",
    "# and title that will give insight into passenger status as well as the age.\n",
    "# Finding Group tickets and changing Ticket fare per group member rather than total\n",
    "#Because loc was used it is better to move to the other Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cabin True\n"
     ]
    }
   ],
   "source": [
    "# Fares now are per-person normalized. Lets see If we can restrore some cabin data from the tickets.\n",
    "for ticket_tag in ticket_tags:\n",
    "    if not np.all(pd.isnull(full[full['Ticket'] == ticket_tag] ['Cabin'])):\n",
    "        if np.any(pd.isnull(full[full['Ticket'] == ticket_tag] ['Cabin'])):\n",
    "            fillnan_value  = full[full['Ticket'] == ticket_tag].Cabin.dropna().mode()[0]\n",
    "            full.loc[full['Ticket'] == ticket_tag,['Cabin']] = full[full['Ticket'] == ticket_tag].Cabin.fillna(fillnan_value)\n",
    "fillnan_value='U'\n",
    "full['Cabin']=full['Cabin'].replace('', fillnan_value)\n",
    "full['Cabin']=full['Cabin'].replace(np.nan, fillnan_value)\n",
    "print('Cabin', full[pd.isnull(full['Cabin'])==True].empty)\n",
    "# with that cabin variable is set and ready to be turned into a categorial \"deck\"d\n",
    "full.Cabin = full.Cabin.map( lambda c : c[0] )\n",
    "cabin_dummies = pd.get_dummies(full['Cabin'], prefix='Cabin') \n",
    "full = pd.concat([full, cabin_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fares and ports time. My guess that ticket fare strongly depends on emberkment port\n",
    "# Also both of the missing values seem to be from the same port\n",
    "common_port=full.Embarked.dropna().mode()[0]\n",
    "full.Embarked = full.Embarked.fillna(common_port)\n",
    "fill_fare_value = full.groupby('Embarked').mean()['Fare'][2]\n",
    "full.Fare = full.Fare.fillna(fill_fare_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Age', 'Cabin', 'Embarked', 'Fare', 'Name', 'Parch', 'PassengerId',\n",
      "       'Pclass', 'Sex', 'SibSp', 'Survived', 'Ticket', 'Cabin_A', 'Cabin_B',\n",
      "       'Cabin_C', 'Cabin_D', 'Cabin_E', 'Cabin_F', 'Cabin_G', 'Cabin_T',\n",
      "       'Cabin_U', 'FamilySize', 'Family_Single', 'Family_Small',\n",
      "       'Family_Large', 'Embarked_C', 'Embarked_Q', 'Embarked_S',\n",
      "       'Pclass_First', 'Pclass_Second', 'Pclass_Third'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Lets work on a putting together SibSp and Parch columns. They represent a family size for a given passenger\n",
    "# Also, it would be preffered to categorize them as propability of survival based on the family size is \n",
    "# not linear, and variable itself takes small range of values (0-7) to model it with curve.\n",
    "full[ 'FamilySize' ] = full[ 'Parch' ] + full[ 'SibSp' ] + 1\n",
    "full[ 'Family_Single' ] = full[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )\n",
    "full[ 'Family_Small' ]  = full[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )\n",
    "full[ 'Family_Large' ]  = full[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )\n",
    "full[ 'Embarked_C' ] = full[ 'Embarked' ].map( lambda s : 1 if s == 'C' else 0 )\n",
    "full[ 'Embarked_Q' ] = full[ 'Embarked' ].map( lambda s : 1 if s == 'Q' else 0 )\n",
    "full[ 'Embarked_S' ] = full[ 'Embarked' ].map( lambda s : 1 if s == 'S' else 0 )\n",
    "full[ 'Pclass_First' ] = full[ 'Pclass' ].map( lambda s : 1 if s == 1 else 0 )\n",
    "full[ 'Pclass_Second' ] = full[ 'Pclass' ].map( lambda s : 1 if s == 2 else 0 )\n",
    "full[ 'Pclass_Third' ] = full[ 'Pclass' ].map( lambda s : 1 if s == 3 else 0 )\n",
    "#full=full.drop(['Parch','SibSp','FamilySize','Ticket','PassengerId'],axis=1, inplace=False)\n",
    "print(full.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "\n",
    "full['Title'] = full.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "full['Title'] = full['Title'].replace(['Capt', 'Col','Dr', 'Major', 'Rev'], 'Crew')\n",
    "full['Title'] = full['Title'].replace(['Don'], 'Mr')\n",
    "full['Title'] = full['Title'].replace(['Mlle','Ms','Dona'],'Miss')\n",
    "full['Title'] = full['Title'].replace(['Mme'], 'Mrs')\n",
    "full['Title'] = full['Title'].replace(['Lady','Countess','Sir', 'Jonkheer'], 'Noble')\n",
    "full[ 'Title_Mr' ] = full[ 'Title' ].map( lambda s : 1 if s == 'Mr' else 0 )\n",
    "full[ 'Title_Miss' ] = full[ 'Title' ].map( lambda s : 1 if s == 'Miss' else 0 )\n",
    "full[ 'Title_Mrs' ] = full[ 'Title' ].map( lambda s : 1 if s == 'Mrs' else 0 )\n",
    "full[ 'Title_Noble' ] = full[ 'Title' ].map( lambda s : 1 if s == 'Noble' else 0 )\n",
    "full[ 'Title_Crew' ] = full[ 'Title' ].map( lambda s : 1 if s == 'Crew' else 0 )\n",
    "full[ 'Sex_male' ] = full[ 'Sex' ].map( lambda s : 1 if s == 'male' else 0 )\n",
    "full[ 'Sex_female' ] = full[ 'Sex' ].map( lambda s : 1 if s == 'female' else 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last step is to fill in Age values. As Title in the name clearly correlates with age of a person (Master)\n",
    "# I would need to calculate grouped median values rather than using median age of a whole data\n",
    "grouped_full =full.dropna().groupby(['Sex', 'Pclass','Title'])\n",
    "grouped_median_full = grouped_full.median()\n",
    "grouped_median_full = grouped_median_full.reset_index()[['Sex', 'Pclass', 'Title',  'Age']]                                                 \n",
    "grouped_median_full.head()\n",
    "def fill_age(row):\n",
    "    condition = (\n",
    "        (grouped_median_full['Sex'] == row['Sex']) & \n",
    "        (grouped_median_full['Title'] == row['Title']) & \n",
    "        (grouped_median_full['Pclass'] == row['Pclass'])\n",
    "    ) \n",
    "    return grouped_median_full[condition]['Age'].values[0]\n",
    "full['Age'] = full.apply(lambda row: fill_age(row) if np.isnan(row['Age']) else row['Age'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Age', 'Fare', 'Survived', 'Cabin_A', 'Cabin_B', 'Cabin_C', 'Cabin_D',\n",
      "       'Cabin_E', 'Cabin_F', 'Cabin_G', 'Cabin_T', 'Cabin_U', 'Family_Single',\n",
      "       'Family_Small', 'Family_Large', 'Embarked_C', 'Embarked_Q',\n",
      "       'Embarked_S', 'Pclass_First', 'Pclass_Second', 'Pclass_Third',\n",
      "       'Title_Mr', 'Title_Miss', 'Title_Mrs', 'Title_Noble', 'Title_Crew',\n",
      "       'Sex_male', 'Sex_female'],\n",
      "      dtype='object')\n",
      "Index(['Age', 'Fare', 'Cabin_A', 'Cabin_B', 'Cabin_C', 'Cabin_D', 'Cabin_E',\n",
      "       'Cabin_F', 'Cabin_G', 'Cabin_T', 'Cabin_U', 'Family_Single',\n",
      "       'Family_Small', 'Family_Large', 'Embarked_C', 'Embarked_Q',\n",
      "       'Embarked_S', 'Pclass_First', 'Pclass_Second', 'Pclass_Third',\n",
      "       'Title_Mr', 'Title_Miss', 'Title_Mrs', 'Title_Noble', 'Title_Crew',\n",
      "       'Sex_male', 'Sex_female'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Time to drop some columns\n",
    "full=full.drop(['Parch', 'SibSp', 'FamilySize', 'Ticket', 'PassengerId',\\\n",
    "                'Embarked','Pclass','Name','Title','Cabin', 'Sex'], axis=1, inplace=False)\n",
    "train=full[:891]\n",
    "test=full[891:].drop(['Survived'],axis=1)\n",
    "print(full.columns)\n",
    "print(test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic reg score:  0.8372615039281706\n",
      "SVC score:  0.8967452300785634\n",
      "Normalized SVC score:  0.8395061728395061\n",
      "20  nn score:  0.8507295173961841\n",
      "Naive Bayes 0.7789001122334456\n",
      "DecisionTree  0.7845117845117845\n",
      "Best DecisionTree  0.7542087542087542\n",
      "Random Forest 0.8529741863075196\n",
      "Accuracy Logistic  0.8095371669004209\n",
      "Accuracy SVC  0.8039270687237027\n",
      "Accuracy NSVC  0.8196353436185133\n",
      "Accuracy NN  0.8022440392706873\n",
      "Accuracy Bayes  0.7450210378681626\n",
      "Accuracy Tree  0.7831697054698457\n",
      "Accuracy Best Tree  0.7640953716690042\n",
      "Accuracy Best forest  0.7896213183730716\n",
      "Best method SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)  with score:  0.8196353436185133\n"
     ]
    }
   ],
   "source": [
    "#actuall fitting of survival\n",
    "acc_score = make_scorer(accuracy_score)\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.8)\n",
    "x_norm = x_survival = train.drop('Survived',axis=1)\n",
    "test_norm=test\n",
    "y_survival = train['Survived']\n",
    "scaler=StandardScaler()\n",
    "\n",
    "#Predictors\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_norm, y_survival)\n",
    "print('logistic reg score: ',logreg.score(x_survival, y_survival))\n",
    "\n",
    "svc_surv=SVC(gamma=0.1,C=1)\n",
    "svc_surv.fit(x_survival, y_survival)\n",
    "print('SVC score: ',svc_surv.score(x_survival, y_survival))\n",
    "\n",
    "x_norm[['Fare','Age']] = scaler.fit_transform(x_norm[['Fare','Age']])\n",
    "svc_nsurv = SVC(gamma=1e-1,C=1)\n",
    "svc_nsurv.fit(x_norm, y_survival)\n",
    "print('Normalized SVC score: ',svc_nsurv.score(x_survival, y_survival))\n",
    "\n",
    "knn = KNeighborsClassifier(algorithm='auto', leaf_size=26, metric='minkowski', \n",
    "                           metric_params=None, n_jobs=1, n_neighbors=6, p=2, \n",
    "                           weights='uniform')\n",
    "knn.fit(x_norm, y_survival)\n",
    "print(k,' nn score: ',knn.score(x_survival, y_survival))\n",
    "\n",
    "gaussian = GaussianNB()\n",
    "gaussian.fit(x_survival, y_survival)\n",
    "print('Naive Bayes',gaussian.score(x_survival, y_survival))\n",
    "\n",
    "bestdecision_tree = DecisionTreeClassifier(max_depth=10, max_features='log2',\\\n",
    "                                           max_leaf_nodes=9, min_samples_leaf=9)\n",
    "decision_tree = DecisionTreeClassifier(max_depth=17, max_features='log2',\\\n",
    "                                       max_leaf_nodes=8, min_samples_leaf=8)\n",
    "decision_tree.fit(x_survival, y_survival)\n",
    "bestdecision_tree.fit(x_survival, y_survival)\n",
    "print('DecisionTree ',decision_tree.score(x_survival, y_survival))\n",
    "print('Best DecisionTree ',bestdecision_tree.score(x_survival, y_survival))\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=205, max_depth=7, min_samples_split=6,\\\n",
    "                                       max_features='log2', min_samples_leaf=6)\n",
    "random_forest.fit(x_survival, y_survival)\n",
    "print('Random Forest',random_forest.score(x_survival, y_survival))\n",
    "\n",
    "method=[\n",
    "        logreg, svc_surv, svc_nsurv,\n",
    "        knn ,gaussian, decision_tree,\n",
    "        bestdecision_tree, random_forest\n",
    "        ]\n",
    "scores=np.zeros(8)\n",
    "scores[0] = cross_val_score(logreg,x_survival,y_survival,scoring=acc_score,cv=cv).mean()\n",
    "scores[1] = cross_val_score(svc_surv,x_survival,y_survival,scoring=acc_score,cv=cv).mean()\n",
    "scores[2] = cross_val_score(svc_surv,x_norm,y_survival,scoring=acc_score,cv=cv).mean()\n",
    "scores[3] = cross_val_score(knn,x_survival,y_survival,scoring=acc_score,cv=cv).mean()\n",
    "scores[4] = cross_val_score(gaussian,x_survival,y_survival,scoring=acc_score,cv=cv).mean()\n",
    "scores[5] = cross_val_score(decision_tree,x_survival,y_survival,scoring=acc_score,cv=cv).mean()\n",
    "scores[6] = cross_val_score(bestdecision_tree,x_survival,y_survival,scoring=acc_score,cv=cv).mean()\n",
    "scores[7] = cross_val_score(random_forest,x_survival,y_survival,scoring=acc_score,cv=cv).mean()\n",
    "print('Accuracy Logistic ', scores[0])\n",
    "print('Accuracy SVC ',scores[1])\n",
    "print('Accuracy NSVC ',scores[2])\n",
    "print('Accuracy NN ',scores[3])\n",
    "print('Accuracy Bayes ',scores[4])\n",
    "print('Accuracy Tree ',scores[5])\n",
    "print('Accuracy Best Tree ',scores[6])\n",
    "print('Accuracy Best forest ',scores[7])\n",
    "methodId=np.argmax(scores)\n",
    "print('Best method', str(method[methodId]), ' with score: ', scores[methodId])\n",
    "test_norm[['Fare','Age']] = scaler.fit_transform(test[['Fare','Age']])\n",
    "predictions = method[methodId].predict(test_norm).astype(int)\n",
    "output = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions})\n",
    "output.to_csv('titanic-predictions.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 180 candidates, totalling 1800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   24.7s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1800 out of 1800 | elapsed:  4.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters for method are {'n_estimators': 205, 'min_samples_leaf': 6, 'max_depth': 7, 'min_samples_split': 6} with a score of 0.7964\n"
     ]
    }
   ],
   "source": [
    "#parameters fiddling\n",
    "#x_survival[['Fare','Age']] = scaler.fit_transform(x_survival[['Fare','Age']])\n",
    "#C_range = np.logspace(-2,7, 10)\n",
    "#gamma_range = np.logspace(-3,3, 7)\n",
    "md_range=np.arange(1,10); md_range[0] = 6\n",
    "#mxf_range=np.arange(1,18); mxf_range[0] = 12\n",
    "#lnodes_range=np.arange(1,10); lnodes_range[0] = 9\n",
    "#msleaf_range=np.arange(10); msleaf_range[0] = 8\n",
    "forest_range=np.arange(205,210)\n",
    "sample_split=np.arange(6,7)\n",
    "sample_leaf=np.arange(6,10)\n",
    "boot_range=[True]\n",
    "#SVC_param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "#DT_param_grid = dict( max_depth=md_range,max_features=mxf_range,max_leaf_nodes=lnodes_range,min_samples_leaf=msleaf_range)\n",
    "RF_param_grid=dict(n_estimators=forest_range, max_depth=md_range,\\\n",
    "                   min_samples_split=sample_split, min_samples_leaf=sample_leaf)\n",
    "cv = StratifiedShuffleSplit(n_splits=10, test_size=0.9)\n",
    "r_f=RandomForestClassifier()\n",
    "grid = GridSearchCV(r_f, param_grid=RF_param_grid, cv=cv, verbose=1, n_jobs=-1,  scoring=acc_score)\n",
    "grid.fit(x_survival, y_survival)\n",
    "print(\"The best parameters for method are %s with a score of %0.4f\"\n",
    "     % (grid.best_params_, grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
